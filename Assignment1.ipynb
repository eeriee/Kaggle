{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name:** WANG XI\n",
    "\n",
    "**EID:** xwang258\n",
    "\n",
    "**Kaggle Team Name:** eeriee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS4487 - Assignment 1 - Movie Review Sentiment Analysis\n",
    "Due date: Oct 12, 2015 11:59pm HKT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "In this assignment, the task is predict the sentiment of a movie review sentence.  For example, the review _\"A very charming film with wonderful sentiment and heart\"_ is positive, while the review _\"this is probably the most irritating show I have ever seen in my entire life\"_ is negative.  Your goal is to train a classifier to predict whether a sentence is positive or negative sentiment.\n",
    "\n",
    "\n",
    "## Methodology\n",
    "You need to train classifiers using the training data, and then predict on the test data. You are free to choose the feature extraction method and classifier algorithm.  You are free to use methods that were not introduced in class.  You should probably do cross-validation to select a good parameters.\n",
    "\n",
    "\n",
    "## Evaluation on Kaggle\n",
    "\n",
    "You need to submit your test predictions to Kaggle for evaluation.  50% of the test data will be used to show your ranking on the live leaderboard.  After the assignment deadline, the remaining 50% will be used to calculate your final ranking. The entry with the highest final ranking will win a prize!\n",
    "\n",
    "To submit to Kaggle you need to create an account, and use the competition invitation that will be posted on Canvas.\n",
    "\n",
    "**Note:** You can only submit 2 times per day to Kaggle!\n",
    "\n",
    "## What to hand in\n",
    "You need to turn in the following things:\n",
    "\n",
    "1. This ipynb file with your source code and documentation.\n",
    "2. Your final submission file to Kaggle.\n",
    "\n",
    "Files should be uploaded to Assignment 1 on Canvas.\n",
    "\n",
    "## Grading\n",
    "The marks of the assignment are distributed as follows:\n",
    "- 50% - Results using various classifiers and feature representations.\n",
    "- 30% - Trying out feature representations (e.g. adding additional features) or classifiers not used in the tutorials.\n",
    "- 20% - Quality of the written report.  More points for insightful observations and analysis.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data is in the text file `imdb_train.txt`.  The training text data is in the format: `sentence \\t label\\n`\n",
    "\n",
    "The label is either 1 for positive sentiment or 0 for negative sentiment.\n",
    "\n",
    "The testing data is in the text file `imdb_test.txt`. The test text data is in the format: `sentence\\n`\n",
    "\n",
    "Kaggle submission files are CSV files: \n",
    "\n",
    "<pre>\n",
    "Id,Prediction\n",
    "1,0\n",
    "2,0\n",
    "3,1\n",
    "...\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are two helpful functions for reading the text data and writing the Kaggle submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import IPython.core.display         \n",
    "# setup output image format (Chrome works best)\n",
    "IPython.core.display.set_matplotlib_formats(\"svg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from numpy import *\n",
    "from sklearn import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy import stats\n",
    "import nltk\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import string\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import IPython.utils.warn as warn\n",
    "import csv\n",
    "random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_text_data(fname):\n",
    "    f = open(fname, 'r')\n",
    "    contents = f.read()\n",
    "    f.close()\n",
    "\n",
    "    lines = contents.split(\"\\n\")\n",
    "    txtdata = []\n",
    "    classes = []\n",
    "    for l in lines:\n",
    "        if len(l)>0:\n",
    "            tmp = l.split(\"\\t\")\n",
    "            txtdata.append(tmp[0].strip())\n",
    "            if (len(tmp)>1):\n",
    "                classes.append(int(tmp[1]))\n",
    "    if (len(classes)>0) and (len(txtdata) != len(classes)):        \n",
    "        warn.error(\"mismatched length!\")\n",
    "    \n",
    "    return (txtdata, asarray(classes))\n",
    "\n",
    "def write_csv_kaggle_sub(fname, Y):\n",
    "    # fname = file name\n",
    "    # Y is a list/array with class entries\n",
    "    \n",
    "    # header\n",
    "    tmp = [['Id', 'Prediction']]\n",
    "    \n",
    "    # add ID numbers for each Y\n",
    "    for (i,y) in enumerate(Y):\n",
    "        tmp2 = [(i+1), int(y)]\n",
    "        tmp.append(tmp2)\n",
    "        \n",
    "    # write CSV file\n",
    "    f = open(fname, 'wb')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(tmp)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOUR CODE and DOCUMENTATION HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "(traintxt, trainY) = read_text_data(\"imdb_train.txt\")\n",
    "(testtxt, _)       = read_text_data(\"imdb_test.txt\")\n",
    "\n",
    "print len(traintxt)\n",
    "print len(testtxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2831\n"
     ]
    }
   ],
   "source": [
    "tmp = feature_extraction.text.CountVectorizer()\n",
    "trainXtmp = tmp.fit_transform(traintxt)\n",
    "print len(tmp.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If only using the default tokenizer for the feature extraction, there are 2831 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2600\n"
     ]
    }
   ],
   "source": [
    "stp = feature_extraction.text.CountVectorizer(stop_words = 'english')\n",
    "trainXstp = stp.fit_transform(traintxt)\n",
    "print len(stp.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After excluding the default stop words, there are 2600 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the NB Bernoulli model\n",
    "alphasb = logspace(-1,0,30)\n",
    "vocasb = range(1000,3000,200)\n",
    "avgscoresb = empty((len(alphasb), len(vocasb)))\n",
    "\n",
    "for i,al in enumerate(alphasb):\n",
    "    for j,voca in enumerate(vocasb):    \n",
    "        cntvect = feature_extraction.text.CountVectorizer(stop_words = 'english', max_features=voca)\n",
    "        trainXb = cntvect.fit_transform(traintxt)\n",
    "        bmodel = naive_bayes.BernoulliNB(alpha=al)\n",
    "        myscoreb = cross_validation.cross_val_score(bmodel, trainXb, trainY, cv=5)\n",
    "        avgscoresb[i,j] = mean(myscoreb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size =  1200\n",
      "max acc of cross-validation = 0.807777777778\n",
      "acc of bernoulli =  0.938888888889\n"
     ]
    }
   ],
   "source": [
    "bestib = argmax(avgscoresb)\n",
    "\n",
    "(bestiab, bestivb) = unravel_index(bestib, avgscoresb.shape)\n",
    "bestab = alphasb[bestiab]\n",
    "bestvb = vocasb[bestivb]\n",
    "print \"vocabulary size = \", bestvb\n",
    "print \"max acc of cross-validation =\", avgscoresb[bestiab, bestivb]\n",
    "\n",
    "cntvect = feature_extraction.text.CountVectorizer(stop_words = 'english', max_features=bestvb)\n",
    "trainXb = cntvect.fit_transform(traintxt)\n",
    "bmodel = naive_bayes.BernoulliNB(alpha=bestab)\n",
    "bmodel.fit(trainXb, trainY)\n",
    "predTrainYb = bmodel.predict(trainXb)\n",
    "\n",
    "print \"acc of bernoulli = \", mean(predTrainYb == trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the words such as 'like' that are very common in the document can be important, it is not good to use idf to downscale those common words. Therefore I use tf rather than tf-idf for feature extration in the Naive Bayes Multinomial Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tf\n",
    "alphas = logspace(-1,0,30)\n",
    "vocas = range(1000,3000,200)\n",
    "avgscores = empty((len(alphas), len(vocas)))\n",
    "\n",
    "for i,al in enumerate(alphas):\n",
    "    for j,voca in enumerate(vocas):     \n",
    "        tfvect = feature_extraction.text.TfidfVectorizer(use_idf=False, stop_words = 'english', max_features=voca)\n",
    "        trainXtf = tfvect.fit_transform(traintxt)\n",
    "        mmodel_tf = naive_bayes.MultinomialNB(alpha=al)      \n",
    "        myscore = cross_validation.cross_val_score(mmodel_tf, trainXtf, trainY, cv=5)\n",
    "        avgscores[i,j] = mean(myscore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size =  1400\n",
      "max acc of cross-validation =  0.813333333333\n",
      "acc of tf-idf =  0.948888888889\n"
     ]
    }
   ],
   "source": [
    "besti = argmax(avgscores)\n",
    "\n",
    "(bestia, bestiv) = unravel_index(besti, avgscores.shape)\n",
    "besta = alphas[bestia]\n",
    "bestv = vocas[bestiv]\n",
    "print \"vocabulary size = \", bestv\n",
    "print \"max acc of cross-validation = \", avgscores[bestia,bestiv]\n",
    "\n",
    "tfvect = feature_extraction.text.TfidfVectorizer(use_idf=False, max_features=bestv)\n",
    "trainXtf = tfvect.fit_transform(traintxt)\n",
    "    \n",
    "mmodel_tf = naive_bayes.MultinomialNB(alpha=besta)\n",
    "mmodel_tf.fit(trainXtf, trainY)\n",
    "\n",
    "predtrainYtf = mmodel_tf.predict(trainXtf)\n",
    "\n",
    "print \"acc of tf-idf = \", mean(predtrainYtf==trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of features is 1200 in both model, which means 1400 features has been removed. Although the training accuracy is high for both two models (NB multinomial model has a higher accuracy than the NB bernoulli model), they have a relatively bad performance on the testing data and show the same 78% accuracy (on Kaggle). Some important features are likely to be removed.\n",
    "\n",
    "Try different classifiers to see the performance of the selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc of lr = 0.975555555556\n"
     ]
    }
   ],
   "source": [
    "#logistic regression\n",
    "logreg = linear_model.LogisticRegressionCV(Cs=logspace(-4,1,20), cv=5)\n",
    "logreg.fit(trainXb, trainY)\n",
    "\n",
    "# predict from the model\n",
    "predYreg = logreg.predict(trainXb)\n",
    "\n",
    "# calculate accuracy\n",
    "print \"acc of lr =\", mean(trainY==predYreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training accuracy of logistic regression model is higher than the previous two models. However, the testing accuracy is only 74% on Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After searching for a more robust model, I find out that SGD has been successfully applied to large-scale and sparse machine learning problems often encountered in text classification. So I try SGD classification for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SGD classifier\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier()),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_features': range(1000,2000,200),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__alpha': logspace(-1,0,10),\n",
    "    #'clf__penalty': ('l2', 'elasticnet'),\n",
    "    #'clf__n_iter': (10, 20),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done  50 jobs       | elapsed:   12.2s\n",
      "[Parallel(n_jobs=1)]: Done 200 jobs       | elapsed:   52.1s\n",
      "[Parallel(n_jobs=1)]: Done 450 jobs       | elapsed:  2.0min\n",
      "[Parallel(n_jobs=1)]: Done 800 jobs       | elapsed:  3.4min\n",
      "[Parallel(n_jobs=1)]: Done 1200 out of 1200 | elapsed:  5.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "('pipeline:', ['vect', 'tfidf', 'clf'])\n",
      "Fitting 3 folds for each of 400 candidates, totalling 1200 fits\n",
      "Best score: 0.708\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.21544346900318834\n",
      "\ttfidf__norm: 'l2'\n",
      "\ttfidf__use_idf: True\n",
      "\tvect__max_features: 1000\n",
      "\tvect__ngram_range: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # find the best parameters for both the feature extraction and the\n",
    "    # classifier\n",
    "    print(\"Performing grid search...\")\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=1, verbose=1)\n",
    "\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    grid_search.fit(traintxt, trainY)\n",
    "        \n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc of SGD =  0.874444444444\n"
     ]
    }
   ],
   "source": [
    "pipeline.set_params(clf__alpha=0.21544346900318834,\n",
    "        tfidf__norm='l2',\n",
    "        tfidf__use_idf=True,\n",
    "        vect__max_features=1000,\n",
    "        vect__ngram_range=(1, 1))\n",
    "pipeline.fit(traintxt, trainY)\n",
    "predPip = pipeline.predict(traintxt) \n",
    "\n",
    "print \"acc of SGD = \", mean(predPip == trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since SGD requires a number of hyperparameters such as the regularization parameter, it is hard to tune the parameters so as to find out the best solution. In addtion, the training accuracy and max accuracy of cross validation is relatively low. As a result, the testing accuracy on Kaggle is very low, around 70%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore using different classifiers cannot increase the accuracy. The way to optimize the accuracy is to find out a better feature extraction method. I filter unimportant features and add other features in the following ways and use NB Bernoulli Model to test the performance of new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset(['all', 'six', 'less', 'being', 'indeed', 'over', 'move', 'anyway', 'four', 'not', 'own', 'through', 'yourselves', 'fify', 'where', 'mill', 'only', 'find', 'before', 'one', 'whose', 'system', 'how', 'somewhere', 'with', 'thick', 'show', 'had', 'enough', 'should', 'to', 'must', 'whom', 'seeming', 'under', 'ours', 'has', 'might', 'thereafter', 'latterly', 'do', 'them', 'his', 'around', 'than', 'get', 'very', 'de', 'none', 'cannot', 'every', 'whether', 'they', 'front', 'during', 'thus', 'now', 'him', 'nor', 'name', 'several', 'hereafter', 'always', 'who', 'cry', 'whither', 'this', 'someone', 'either', 'each', 'become', 'thereupon', 'sometime', 'side', 'two', 'therein', 'twelve', 'because', 'often', 'ten', 'our', 'eg', 'some', 'back', 'up', 'go', 'namely', 'towards', 'are', 'further', 'beyond', 'ourselves', 'yet', 'out', 'even', 'will', 'what', 'still', 'for', 'bottom', 'mine', 'since', 'please', 'forty', 'per', 'its', 'everything', 'behind', 'un', 'above', 'between', 'it', 'neither', 'seemed', 'ever', 'across', 'she', 'somehow', 'be', 'we', 'full', 'never', 'sixty', 'however', 'here', 'otherwise', 'were', 'whereupon', 'nowhere', 'although', 'found', 'alone', 're', 'along', 'fifteen', 'by', 'both', 'about', 'last', 'would', 'anything', 'via', 'many', 'could', 'thence', 'put', 'against', 'keep', 'etc', 'amount', 'became', 'ltd', 'hence', 'onto', 'or', 'con', 'among', 'already', 'co', 'afterwards', 'formerly', 'within', 'seems', 'into', 'others', 'while', 'whatever', 'except', 'down', 'hers', 'everyone', 'done', 'least', 'another', 'whoever', 'moreover', 'couldnt', 'throughout', 'anyhow', 'yourself', 'three', 'from', 'her', 'few', 'together', 'top', 'there', 'due', 'been', 'next', 'anyone', 'eleven', 'much', 'call', 'therefore', 'interest', 'then', 'thru', 'themselves', 'hundred', 'was', 'sincere', 'empty', 'more', 'himself', 'elsewhere', 'mostly', 'on', 'fire', 'am', 'becoming', 'hereby', 'amongst', 'else', 'part', 'everywhere', 'too', 'herself', 'former', 'those', 'he', 'me', 'myself', 'made', 'twenty', 'these', 'bill', 'cant', 'us', 'until', 'besides', 'nevertheless', 'below', 'anywhere', 'nine', 'can', 'of', 'toward', 'my', 'something', 'and', 'whereafter', 'whenever', 'give', 'almost', 'wherever', 'is', 'describe', 'beforehand', 'herein', 'an', 'as', 'itself', 'at', 'have', 'in', 'seem', 'whence', 'ie', 'any', 'fill', 'again', 'hasnt', 'inc', 'thereby', 'thin', 'no', 'perhaps', 'latter', 'meanwhile', 'when', 'detail', 'same', 'wherein', 'beside', 'also', 'that', 'other', 'take', 'which', 'becomes', 'you', 'if', 'nobody', 'see', 'though', 'may', 'after', 'upon', 'most', 'hereupon', 'eight', 'but', 'serious', 'nothing', 'such', 'your', 'why', 'a', 'off', 'whereby', 'third', 'i', 'whole', 'noone', 'sometimes', 'well', 'amoungst', 'yours', 'their', 'rather', 'without', 'so', 'five', 'the', 'first', 'whereas', 'once'])\n"
     ]
    }
   ],
   "source": [
    "print stp.get_stop_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, in the default stop-word list, some important words like 'no' are excluded from the features, which affects the accuracy of classification. Therefore it is better to create a new stop-word list instead of the default one. After growing the stopwords list by iteratively analyzing the top features in the algorithm for words that shouldn’t be in there, the new stop-word list is as below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stp_list = ['a', 'an', 'the','there', 'theres', 'what', 'where','when', 'which', 'who', 'whom', 'then', 'those', 'which',\n",
    "            'do','does', 'did', 'had', 'has', 'have', 'can', 'are', 'was', 'were', 'is','be','thats', 'this', 'that', 'such',\n",
    "            'they',  'he','she',  'shes', 'your','youll','youself', 'youd','youve', 'me','hes', 'i','ill', 'ive', 'id', 'im', 'it',\n",
    "            'them', 'its', 'themselves','itself','us', 'we', 'him', 'her', 'his','youre','my', 'our', 'you','am', 'having',\n",
    "            'camera', 'same', 'show', 'some', 'action', 'actor', 'actors', 'actress', 'actresses', 'film', 'films','movie', 'movies', 'people',\n",
    "            'to', 'written', 'for','by','as', 'also', 'with','of', 'end', 'or', 'and', 'in', 'about', 'into', 'yet',  'now', 'from']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw features tokenized by the existed function contain punctuation which is obviously useless for classification. So new tokenizer should be written. The written tokenizer removes the punctuation and then removes designed stop-word list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation]) #move punctuation\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    filtered_tokens = []\n",
    "        \n",
    "    for t in tokens:\n",
    "        if t not in stp_list:\n",
    "            filtered_tokens.append(t)\n",
    "            \n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is noticeable that two-related words (Bigrams) can result in different sentiment from two single words (Unigrams). For example, 'too many' can be in a negative sentiment while 'too' and 'many' are likely to be in a positive sentiment. Therefore, adding Bigrams can provide more information for the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, many Bigrams are too trivial: they only appear once in the whole training data. Therefore I'd like to limit the min_df as 0.002 (2/900 is slightly larger than 0.002), which also saves the time to select the best parameter of max features. The min_df also remove some Unigram features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#the NB Bernoulli model\n",
    "alphasb = logspace(-1,0,30)\n",
    "avgscoresb = empty(len(alphasb))\n",
    "\n",
    "cntvect = feature_extraction.text.CountVectorizer(tokenizer =  tokenize, ngram_range=(1, 2), min_df=0.002)\n",
    "trainXb = cntvect.fit_transform(traintxt)\n",
    "\n",
    "for i,al in enumerate(alphasb):       \n",
    "        bmodel = naive_bayes.BernoulliNB(alpha=al)\n",
    "        myscoreb = cross_validation.cross_val_score(bmodel, trainXb, trainY, cv=5)\n",
    "        avgscoresb[i] = mean(myscoreb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max acc of cross-validation = 0.823333333333\n",
      "acc of bernoulli =  0.936666666667\n"
     ]
    }
   ],
   "source": [
    "bestib = argmax(avgscoresb)\n",
    "bestab = alphasb[bestib]\n",
    "\n",
    "\n",
    "print \"max acc of cross-validation =\", avgscoresb[bestib]\n",
    "\n",
    "bmodel = naive_bayes.BernoulliNB(alpha=bestab)\n",
    "bmodel.fit(trainXb, trainY)\n",
    "predTrainYb = bmodel.predict(trainXb)\n",
    "\n",
    "print \"acc of bernoulli = \", mean(predTrainYb == trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The max accuracy of cross-validation is larger than previous NB Bernoulli model but the training accuracy decreases slightly. However, the testing accuracy on Kaggle now increases to 84%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1210\n"
     ]
    }
   ],
   "source": [
    "print len(cntvect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of features is also reduced to 1210."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, same words occur in the features as different format. As the grammatical placement of words is an irrelevant feature to the classifier, it is important to transform those words into the same format. I try both stemming and lemmatization and notice that stemming and lemmatization both increase the accuracy.\n",
    "\n",
    "Stemming and lemmatization reduces not only the redundant features that the algorithm generated by the training set, but also the chances of encountering new words that the algorithm has not been trained on. since derivatives are transformed to a single format, the practical accuracy increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "def tokenize_and_stem(text):\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation]) #move punctuation\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    filtered_tokens = []\n",
    "        \n",
    "    for t in tokens:\n",
    "        if t not in stp_list:\n",
    "            filtered_tokens.append(t)\n",
    "            \n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Lemmatization\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def is_noun(tag):\n",
    "    return tag.startswith('N')\n",
    "\n",
    "\n",
    "def is_verb(tag):\n",
    "    return tag.startswith('V')\n",
    "\n",
    "\n",
    "def is_adverb(tag):\n",
    "    return tag.startswith('R')\n",
    "\n",
    "\n",
    "def is_adjective(tag):\n",
    "    return tag.startswith('J')\n",
    "\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    if is_adjective(tag):\n",
    "        return wn.ADJ\n",
    "    elif is_noun(tag):\n",
    "        return wn.NOUN\n",
    "    elif is_adverb(tag):\n",
    "        return wn.ADV\n",
    "    elif is_verb(tag):\n",
    "        return wn.VERB\n",
    "    return wn.NOUN\n",
    "\n",
    "def tokenize_and_lemma(text):\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation]) #move punctuation\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    lemmas = []\n",
    "    \n",
    "    filtered_tokens = []\n",
    "        \n",
    "    for t in tokens:\n",
    "        if t not in stp_list:\n",
    "            filtered_tokens.append(t)\n",
    "            \n",
    "    for (t, tag) in nltk.pos_tag(filtered_tokens):\n",
    "        lemmas.append(WordNetLemmatizer().lemmatize(t, penn_to_wn(tag)))\n",
    "                \n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take stemming as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the NB Bernoulli model\n",
    "alphasb = logspace(-1,0,30)\n",
    "avgscoresb = empty(len(alphasb))\n",
    "\n",
    "cntvect = feature_extraction.text.CountVectorizer(tokenizer =  tokenize_and_stem, ngram_range=(1, 2), min_df=0.002)\n",
    "trainXb = cntvect.fit_transform(traintxt)\n",
    "testXb = cntvect.transform(testtxt)\n",
    "\n",
    "for i,al in enumerate(alphasb):       \n",
    "        bmodel = naive_bayes.BernoulliNB(alpha=al)\n",
    "        myscoreb = cross_validation.cross_val_score(bmodel, trainXb, trainY, cv=5)\n",
    "        avgscoresb[i] = mean(myscoreb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max acc of cross-validation = 0.822222222222\n",
      "acc of bernoulli =  0.944444444444\n"
     ]
    }
   ],
   "source": [
    "bestib = argmax(avgscoresb)\n",
    "bestab = alphasb[bestib]\n",
    "\n",
    "\n",
    "print \"max acc of cross-validation =\", avgscoresb[bestib]\n",
    "\n",
    "bmodel = naive_bayes.BernoulliNB(alpha=bestab)\n",
    "bmodel.fit(trainXb, trainY)\n",
    "predTrainYb = bmodel.predict(trainXb)\n",
    "predY = bmodel.predict(testXb)\n",
    "print \"acc of bernoulli = \", mean(predTrainYb == trainY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1204\n"
     ]
    }
   ],
   "source": [
    "print len(cntvect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features are further reduced. The training accuracy increases and the testing accuracy on Kaggle becomes 90% (the testing accuracy of the classifier using lemmatization to produce features is 88%).\n",
    "\n",
    "Applying the designed stop-word list, tokenize_and_stem function, bigrams to the tf model, the result is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write your predictions on the test set \n",
    "predY = bmodel.predict(testXb)  # for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "write_csv_kaggle_sub(\"my_submission.csv\", predY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
